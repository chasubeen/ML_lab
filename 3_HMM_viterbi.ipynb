{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVZzo0LrKvF3HPFYF6B+81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chasubeen/ML_lab/blob/main/3_HMM_viterbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0c5QVs-_FwjX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Preprocessing**"
      ],
      "metadata": {
        "id": "90ZRKh3dERdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1-1. Load data**\n",
        "- CoNLL-2000 file을 읽어와 word(x)와 tag(z) 처리\n",
        "- 특수 품사(`remove_pos`) 제거"
      ],
      "metadata": {
        "id": "_Gx_9utFkNiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUa6BHK3j0_6",
        "outputId": "04138a75-a630-4847-9c70-20c786ea198d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data path\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/ML_lab/3_HMM/data/train.txt\"\n",
        "test_path = \"/content/drive/MyDrive/ML_lab/3_HMM/data/test.txt\""
      ],
      "metadata": {
        "id": "TfZ8ZwHjjw6t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_pos = ['#', '$', \"''\", '(', ')', ',', '.', ':', '``', 'POS', 'SYM']"
      ],
      "metadata": {
        "id": "h-K1bm9ALqDa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Read file\n",
        "\n",
        "def read_file(path):\n",
        "  raw = open(path, 'r').read().strip().split('\\n') # 파일을 읽어와 문장마다 나누기\n",
        "  words, tags = [], []\n",
        "\n",
        "  for line in raw:\n",
        "    if line.strip(): # 빈 줄이 아니면\n",
        "      parts = line.split() # 열 분할\n",
        "\n",
        "      if len(parts) == 3:\n",
        "        word, pos, chunk = parts\n",
        "        if pos not in remove_pos: # 특정 POS는 제거\n",
        "          words.append(word.lower()) # 소문자로 변환\n",
        "          tags.append(pos)\n",
        "        else:\n",
        "          continue # 적절한 tag가 없는 경우는 건너뜀\n",
        "\n",
        "  return words, tags"
      ],
      "metadata": {
        "id": "yLrEXl2J0qSU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_words, train_tags = read_file(train_path)\n",
        "test_words, test_tags = read_file(test_path)"
      ],
      "metadata": {
        "id": "gtY4NX2UJ9wP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1-2. Preprocess Data**"
      ],
      "metadata": {
        "id": "BkJqEkdx3zsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **a) 특수 문자 제거**"
      ],
      "metadata": {
        "id": "7PYmlGNBfy6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def only_alphabet(words, tags):\n",
        "  processed_words = []\n",
        "  processed_tags = []\n",
        "\n",
        "  for word, tag in zip(words, tags):\n",
        "    alphabetic = re.sub('[^a-z]', '', word)  # 알파벳만 남기기\n",
        "    if alphabetic:\n",
        "      processed_words.append(alphabetic)\n",
        "      processed_tags.append(tag)\n",
        "\n",
        "  return processed_words, processed_tags"
      ],
      "metadata": {
        "id": "2A5prIu3f6G2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_words, train_tags = only_alphabet(train_words, train_tags)\n",
        "test_words, test_tags = only_alphabet(test_words, test_tags)"
      ],
      "metadata": {
        "id": "BQD1CU3HgQaS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **b) POS Tag 통합 및 변경**"
      ],
      "metadata": {
        "id": "3RLlPdyogbSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_tags(tags):\n",
        "  modified_tags = []\n",
        "\n",
        "  for tag in tags:\n",
        "    if tag in ['JJ', 'JJR', 'JJS']:\n",
        "      modified_tags.append('ADJ')\n",
        "    elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "      modified_tags.append('N')\n",
        "    elif tag in ['PRP', 'PRP$', 'WDT', 'WP', 'WP$']:\n",
        "      modified_tags.append('PN')\n",
        "    elif tag in ['RB', 'RBR', 'RBS', 'WRB']:\n",
        "      modified_tags.append('AD')\n",
        "    elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "      modified_tags.append('V')\n",
        "    else:\n",
        "      modified_tags.append(tag)  # 다른 경우는 그대로 유지\n",
        "\n",
        "  return modified_tags"
      ],
      "metadata": {
        "id": "IKYQMZudgjor"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tags = modify_tags(train_tags)\n",
        "test_tags = modify_tags(test_tags)"
      ],
      "metadata": {
        "id": "KPe5fAKwg2rM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **c) make dictionary**"
      ],
      "metadata": {
        "id": "wXFMcgL1qkhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dict(tags, words):\n",
        "  tag_to_index = {tag: idx for idx, tag in enumerate(set(tags))}\n",
        "  vocab_to_index = {word: idx for idx, word in enumerate(set(words))}\n",
        "\n",
        "  return tag_to_index, vocab_to_index"
      ],
      "metadata": {
        "id": "ZylZeb4dqs0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tag_dict, train_word_dict = make_dict(train_tags, train_words)\n",
        "test_tag_dict, test_word_dict = make_dict(test_tags, test_words)"
      ],
      "metadata": {
        "id": "eYAmayl8roIM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **e) index 변환**"
      ],
      "metadata": {
        "id": "eDANFZv6-umk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### () to index\n",
        "\n",
        "def to_index(objs, obj_dict):\n",
        "  return [obj_dict[obj] for obj in objs if obj in obj_dict]"
      ],
      "metadata": {
        "id": "WJCyTkpusMKO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tag_idx = to_index(train_tags, train_tag_dict)\n",
        "test_tag_idx = to_index(test_tags, test_tag_dict)"
      ],
      "metadata": {
        "id": "rKSaWMvj_u35"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_idx = to_index(train_words, train_word_dict)\n",
        "test_data_idx = to_index(test_words, test_word_dict)"
      ],
      "metadata": {
        "id": "8I-UZgbhsmrQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1-3. 최종 데이터 확인**"
      ],
      "metadata": {
        "id": "QdxBUoO4lUSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tags[:5])\n",
        "print()\n",
        "print(train_words[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOnADvaClXam",
        "outputId": "c062b4aa-056f-420b-db6a-14882d8b4517"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['N', 'IN', 'DT', 'N', 'V']\n",
            "\n",
            "['confidence', 'in', 'the', 'pound', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Modeling**"
      ],
      "metadata": {
        "id": "FmuTQoGqmta5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HMM_viterbi:\n",
        "  def __init__(self, num_states):\n",
        "    self.num_states = num_states # Hidden 상태의 개수(POS 태그 개수)\n",
        "\n",
        "  def fit(self, X, Z, word_size, smoothing=1.0):\n",
        "    ## pi 추정\n",
        "    # Initialization\n",
        "    self.pi = np.zeros(self.num_states)  # initial state probabilities\n",
        "\n",
        "    for z in Z:\n",
        "      self.pi[z[0]] += 1\n",
        "    self.pi = (self.pi + smoothing) / (len(Z) + smoothing * self.num_states)\n",
        "\n",
        "    ## A 추정(transition probabilities matrix)\n",
        "    # Initialization\n",
        "    self.A = np.zeros((self.num_states, self.num_states))\n",
        "\n",
        "    for z in Z:\n",
        "      for i in range(len(z) - 1):\n",
        "        self.A[z[i], z[i + 1]] += 1\n",
        "    self.A = (self.A + smoothing) / (self.A.sum(axis=1, keepdims=True) + smoothing * self.num_states)\n",
        "\n",
        "    ## B 추정(emission probabilities matrix)\n",
        "    # Initialization\n",
        "    self.B = np.zeros((self.num_states, word_size))\n",
        "\n",
        "    for x, z in zip(X, Z):\n",
        "      for xi, zi in zip(x, z):\n",
        "        self.B[zi, xi] += 1\n",
        "    self.B = (self.B + smoothing) / (self.B.sum(axis=1, keepdims=True) + smoothing * word_size)\n",
        "\n",
        "  def calculate_likelihood(self, V, t, z_t):\n",
        "    return V[t-1] + np.log(self.A[:, z_t]) + np.log(self.B[z_t, t])\n",
        "\n",
        "  def update_state(self, V, t):\n",
        "    return np.argmax(V[t-1] + np.log(self.A[:, t]))\n",
        "\n",
        "  def get_latent_path(self, X, num_words):\n",
        "    V = np.zeros((num_words, self.num_states))\n",
        "    state_idx = np.zeros((num_words, self.num_states), dtype=int)\n",
        "\n",
        "    # Initialization\n",
        "    V[0] = np.log(self.pi) + np.log(self.B[:, X[0]])\n",
        "\n",
        "    # Recursion\n",
        "    for t in range(1, num_words):\n",
        "      V[t] = np.max(V[t - 1, :, np.newaxis] + np.log(self.A.T) + np.log(self.B[:, X[t]]), axis=0)\n",
        "      state_idx[t] = np.argmax(V[t - 1, :, np.newaxis] + np.log(self.A.T), axis=0)\n",
        "\n",
        "    # Traceback\n",
        "    z = np.zeros(num_words, dtype=int)\n",
        "    z[-1] = np.argmax(V[-1])\n",
        "    for t in range(num_words - 2, -1, -1):\n",
        "      z[t] = state_idx[t + 1, z[t + 1]]\n",
        "\n",
        "    return z\n",
        "\n",
        "  def predict(self, X):\n",
        "    num_words = len(X)\n",
        "    return self.get_latent_path(X, num_words)"
      ],
      "metadata": {
        "id": "DNrb5c56nmIf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "fWvmMRgjnobu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = len(train_tag_dict)\n",
        "vocab_size = len(train_word_dict)"
      ],
      "metadata": {
        "id": "gmFfpf_rnqm1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HMM_viterbi(num_states = num_states)"
      ],
      "metadata": {
        "id": "PV-Z0ZdxAp5S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터 준비 (각 문장을 인덱스 형태로 변환)\n",
        "\n",
        "train_sequences = [train_data_idx]\n",
        "train_tags_sequences = [train_tag_idx]"
      ],
      "metadata": {
        "id": "waeK0JUWAvrB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothing = 1.0  # 스무딩 파라미터 설정\n",
        "model.fit(train_sequences, train_tags_sequences, vocab_size, smoothing)"
      ],
      "metadata": {
        "id": "pC_Qh_3OA01S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Testing**"
      ],
      "metadata": {
        "id": "7Stw8cxCBKAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# likelihood 계산 및 시각화\n",
        "def calculate_likelihood_sequence(model, X, Z):\n",
        "  likelihoods = []\n",
        "  for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    z = Z[i]\n",
        "    V = np.zeros((len(x), model.num_states))\n",
        "\n",
        "    # Initialization\n",
        "    V[0] = np.log(model.pi) + np.log(model.B[:, x[0]])\n",
        "\n",
        "    # Recursion\n",
        "    for t in range(1, len(x)):\n",
        "      V[t] = np.max(V[t - 1, :, np.newaxis] + np.log(model.A.T) + np.log(model.B[:, x[t]]), axis=0)\n",
        "\n",
        "    # Calculate likelihood for this sequence\n",
        "    likelihood = np.sum(V[-1])\n",
        "    likelihoods.append(likelihood)\n",
        "\n",
        "  return likelihoods"
      ],
      "metadata": {
        "id": "8ZUb1bKvHsYk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 준비\n",
        "test_sequences = [test_data_idx]\n",
        "test_tags_sequences = [test_tag_idx]"
      ],
      "metadata": {
        "id": "ioeC2PHZHxSy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 반복(iteration)마다의 likelihood 계산\n",
        "train_likelihoods = calculate_likelihood_sequence(model, train_sequences, train_tags_sequences)\n",
        "test_likelihoods = calculate_likelihood_sequence(model, test_sequences, test_tags_sequences)"
      ],
      "metadata": {
        "id": "Rx6sZ7C3Hysf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HMFYhjvZH6LO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvxRbI3SH40H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}